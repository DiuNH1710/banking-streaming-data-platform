# ğŸš€ Real-Time Banking Analytics Platform with Kafka, Debezium, Snowflake, dbt & Airflow

_A Complete End-to-End Modern Data Engineering Project_

## ğŸ“˜ Overview

This project builds a fully-functional **real-time banking data platform** using a cloud-native modern data stack.

It simulates real banking operations (customers, accounts, transactions), streams real-time database changes using **Kafka + Debezium**, lands raw data into **MinIO**, orchestrates pipelines with **Apache Airflow**, transforms data with **dbt**, stores analytics models in **Snowflake**, and finally visualizes insights in **Power BI**.

The entire workflow is automated using **CI/CD with GitHub Actions**.

ğŸ‘‰ **This is a production-grade project** that mirrors how real financial institutions build scalable data ecosystems.

## ğŸ—ï¸ System Architecture
![img_22.png](images%2Fimg_22.png)

## âš¡ Technology Stack
| Layer                          | Tools                                 |
| ------------------------------ | ------------------------------------- |
| **OLTP / Source System**       | PostgreSQL                            |
| **CDC Streaming**              | Kafka, Debezium                       |
| **Object Storage (Data Lake)** | MinIO                                 |
| **ETL/ELT Orchestration**      | Apache Airflow                        |
| **Cloud Data Warehouse**       | Snowflake                             |
| **Transformations & Modeling** | dbt (staging, marts, SCD-2 snapshots) |
| **Dashboarding**               | Power BI                              |
| **Automation**                 | GitHub Actions CI/CD                  |
| **Infrastructure**             | Docker & docker-compose               |
| **Data Simulation**            | Python + Faker                        |

## ğŸ¯ Key Capabilities

- Real-time streaming from OLTP database â†’ Data Lake â†’ Snowflake

- End-to-end CDC via **Debezium** (reading WAL logs)

- Automated ingestion pipelines orchestrated using **Airflow**

- Clean and modeled data marts in **dbt**

- Slowly Changing Dimensions **(SCD Type-2)** using **dbt snapshots**

- CI/CD pipelines for dbt (testing, validation, deployment)

- Enterprise-level BI dashboards powered by **Power BI**

- Infrastructure fully containerized using **Docker**

## ğŸ“‚ Repository Structure

```bash
    banking-modern-data/
â”œâ”€â”€ .github/workflows/           # CI/CD pipelines (ci.yml, cd.yml)
â”‚   â”œâ”€â”€ ci.yml                   # Runs dbt tests + linting
â”‚   â””â”€â”€ cd.yml                   # Deploys dbt models to Snowflake
â”‚
â”œâ”€â”€ banking_dbt/                 # dbt project (transforms, marts, snapshots)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/             # Staging models (Bronze â†’ Silver)
â”‚   â”‚   â”œâ”€â”€ marts/               # Facts & Dimensions (Gold layer)
â”‚   â”‚   â””â”€â”€ sources.yml          # Source definitions
â”‚   â”œâ”€â”€ snapshots/               # SCD Type-2 history tracking
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ kafka_to_minio.py        # CDC consumer â†’ MinIO writer
â”‚
â”œâ”€â”€ data-generator/              # Synthetic banking dataset generator
â”‚   â””â”€â”€ faker_generator.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ dags/                    # Airflow DAGs (Snowflake ingestion + snapshots)
â”‚   â”œâ”€â”€ plugins/                 # Airflow plugins (if any)
â”‚   â””â”€â”€ minio/â€¦                  # MinIO volume structure
â”‚
â”œâ”€â”€ kafka-debezium/
â”‚   â””â”€â”€ generate_and_post_connector.py
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ schema.sql               # DDL + seed data for OLTP database
â”‚
â”œâ”€â”€ docker-compose.yml           # Infrastructure (Kafka, Debezium, Airflow, MinIOâ€¦)
â”œâ”€â”€ dockerfile-airflow.dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
## ğŸ§¬ Step-by-Step Implementation
### 1ï¸âƒ£ Data Simulation â€” Realistic Banking Operations

We generate **customers, accounts, and transactions** using Python + Faker.  
The data behaves like a real banking OLTP system with constraints, foreign keys, balances, and transactional behavior.

### âœ” Features
- Customer onboarding  
- Account opening  
- Deposits, withdrawals, transfers  
- Fraud-like transaction patterns  
- Inserted directly into **PostgreSQL OLTP**  
- 
![img_2.png](images%2Fimg_2.png)

### â–¶ï¸ Run the data generator:
```bash
python data-generator/faker_generator.py
```
![img_3.png](images%2Fimg_3.png)

**In Postgres:**
![img_4.png](images%2Fimg_4.png)

### 2ï¸âƒ£ Real-Time CDC with Kafka + Debezium

Debezium monitors **PostgreSQL WAL logs**
â†’ captures **INSERT / UPDATE / DELETE**
â†’ streams changes into **Kafka topics**

Your connector automatically writes raw messages into MinIO through a consumer.


#### â–¶ï¸ Run the Debezium Connector Generator:
```bash 
python kafka-debezium/generate_and_post_connector.py
```

**Kafka topics created:**

- banking_server.public.customers

- banking_server.public.accounts

- banking_server.public.transactions

#### âš ï¸ If Kafka libs fail, fix with:
```bash 
pip install --upgrade six
pip install --upgrade kafka-python
```
### 3ï¸âƒ£ MinIO â€” S3-Compatible Data Lake (Bronze Layer)

Kafka messages are consumed and stored as Parquet files in MinIO.

#### â–¶ï¸ Run the Kafka â†’ MinIO consumer:
```bash
python consumer/kafka_to_minio.py
```

![img_7.png](images%2Fimg_7.png)

#### â–¶ï¸ Required parquet package:
```bash
pip install fastparquet
```
![img_8.png](images%2Fimg_8.png)
![img_9.png](images%2Fimg_9.png)

### 4ï¸âƒ£ Apache Airflow â€” The Orchestration Layer

Airflow automates the entire pipeline:

- Load raw data from MinIO â†’ Snowflake (Bronze)

- Execute dbt models (Silver & Gold)

- Execute dbt snapshots (SCD Type-2)

- Daily scheduling or near real-time orchestration

Airflow DAGs live in:
```bash
docker/dags/
```
![img_10.png](images%2Fimg_10.png)


### 5ï¸âƒ£ Snowflake â€” Cloud Data Warehouse

Snowflake stores analytics data across multiple processing layers:

| Layer      | Description                                 |
| ---------- | ------------------------------------------- |
| **Bronze** | Raw CDC data loaded from MinIO              |
| **Silver** | Cleaned & standardized staging models       |
| **Gold**   | Fact tables, dimensions, and business marts |

<!-- Snowflake DB screenshot -->

![img_11.png](images%2Fimg_11.png)

#### â–¶ï¸ Setup used:

- Warehouse: COMPUTE_WH

- Role: ACCOUNTADMIN

- Default DB: BANKING

- Schema: ANALYTICS

### 6ï¸âƒ£ dbt Models & Transformations

dbt is used for transformations, tests, and snapshots.

#### â–¶ï¸ Install dbt:
```bash
pip install dbt-core
pip install dbt-snowflake
```


#### â–¶ï¸ Initialize the dbt project:
```bash
dbt init banking_dbt
```
Values you configured:
```bash
...
role: accountadmin
warehouse: COMPUTE_WH
database: banking
schema: analytics
threads: 4
```
#### â–¶ï¸ Test Snowflake connection:
```bash
dbt debug
```

##### âœ” dbt Staging Models

Clean and standardize raw CDC data.
![img_15.png](images%2Fimg_15.png)

##### âœ” dbt Marts

- dim_customers

- dim_accounts

- fact_transactions
- 
![img_17.png](images%2Fimg_17.png)

##### âœ” dbt Snapshots (SCD2)

Track history for:

- Customer attributes

- Account attributes

![img_16.png](images%2Fimg_16.png)

#### â–¶ï¸ Run all dbt models:
```bash
dbt run
```


#### â–¶ï¸ Run SCD Type-2 snapshots:
```bash
dbt snapshot
```

#### â–¶ï¸ Run only marts:
```bash
dbt run --select marts
```
### 7ï¸âƒ£ CI/CD with GitHub Actions

Two workflow pipelines: CI and CD.


#### ğŸŸ¦ CI Pipeline (ci.yml)

Triggers on:

- Push to dev

- Pull Request to main

CI performs:

- Setup Python environment

- Install dbt

- Install ruff

- Run dbt compile

- Run dbt test

- Validate code quality before merging

![img_18.png](images%2Fimg_18.png)

#### ğŸŸ© CD Pipeline (cd.yml)

Triggers on:

- Merge PR from dev â†’ main

CD performs:

- Run dbt models on Snowflake production

- Run dbt snapshots

- Run dbt tests

- Deploy transformations automatically

![img_19.png](images%2Fimg_19.png)

### 8ï¸âƒ£ Power BI Dashboard â€” Real-Time Banking Analytics

Power BI connects directly to Snowflake to visualize:

#### ğŸ“Š Insights included:

- Customer growth trends

- Account activity over time

- Transaction insights (deposit / withdraw / transfer)

- Fraud-like anomalies

- SCD2 historical dimension tracking

![img_21.png](images%2Fimg_21.png)

---
## ğŸ“ˆ Final Outcomes

By the end of this project, you will have:

âœ” A full data engineering pipeline running exactly like real banks
âœ” Real-time CDC from PostgreSQL â†’ Kafka â†’ MinIO â†’ Snowflake
âœ” dbt + Snowflake star schema & SCD2 modeling
âœ” Automated CI/CD for dbt
âœ” Airflow orchestrating ingestion & transformations
âœ” A complete Power BI dashboard
âœ” 100% containerized, reproducible environment

---
## ğŸ‘¨â€ğŸ’» Author

**Diu Nguyen**

Data Engineer | Full Stack Developer

ğŸ“§ nguyenhuongdiu1710@gmail.com